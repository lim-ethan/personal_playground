# 테스트 환경 (시나리오에 공통 적용)

- **Python**  
  Version: 3.10

- **Apache Spark**  
  Version: 3.5.0  
  - Notebook 형태로 테스트 진행  
    (Spark Submit 기반 .py 파일 구성 및 .sh Script 구성은 테스트 및 디버깅에 용이하지 못하여 Notebook 형태로 진행)  
  - Spark Package Install & Download (JARS)  
    - `org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.2`  
    - `org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0`  
    - `org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.3.0`  
    - `mysql:mysql-connector-java:5.1.49`  
    - `org.mongodb.spark:mongo-spark-connector_2.12:3.0.1`  
    - `org.elasticsearch:elasticsearch-spark-30_2.12:8.10.2`

- **Apache Kafka**  
  - 단일 브로커 서버로 구성  
  - 토픽 파티션은 1, replica set은 0 (단순 테스트용)  
  - SASL과 같은 보안 정책 적용하지 않음  
  - 컨슈머 그룹은 따로 만들지 않고, 단일 컨슈머로 데이터 컨슘 진행  
  - Kafka UI를 동시에 배포하여 Kafka 상태 확인 가능

- **Apache Iceberg**  
  - Spark Data Format 중 하나로, 보안(컬럼 단위 암복호화 기능 / 데이터 타임 트레블 등)에 좋은 포맷  
  - 추후 Snowflake와 같은 빅데이터 웨어하우스 플랫폼으로 마이그레이션 용이  
    - Snowflake : Iceberg Format  
    - Databricks : Delta Format  
  - 해당 포맷 적용 시 Hive-metastore가 설치되어 있어야 함

- **Dummy File**  
  - [https://codepo8.github.io/json-dummy-data/5.9MB.json](https://codepo8.github.io/json-dummy-data/5.9MB.json)

---

# 시나리오 별 구성도
<p align="center">
  <img src="./00. Local Architecture 구성/01. 시나리오1.png" width="600"/>
</p>

## Fig 1. Apache Spark를 이용한 Local File Streaming

### 시나리오 1 목적

- Spark의 코어 기능 4개 중 Spark Stream 기능 테스트  
- 단순 Spark Stream만을 이용하여 Object File의 데이터를 RDB 형태의 Table 구성

### 시나리오 1 Workflow

- Local 특정 폴더(`./dummy_data`)에 위치한 모든 JSON 파일을 Stream하여 Parquet File 형태로 데이터 저장  
  (Not Iceberg Table, Not Delta Table)

---
<p align="center">
  <img src="./00. Local Architecture 구성/02. 시나리오2.png" width="600"/>
</p>

## Fig 2. Kafka와 Spark를 이용한 데이터 마이그레이션

### 시나리오 2 목적

- Kafka의 Message 기능 테스트 (Pub/Sub)  
- Spark과 연계하여 데이터 Streaming 처리 및 Iceberg Format 테이블 적재

### 시나리오 2 Workflow

- Local 특정 폴더(`./dummy_data`)에 위치한 모든 JSON 파일을 Kafka의 특정 Topic(`test-topic`)에 Publish 진행  
- Kafka의 특정 토픽(`test-topic`)에 Publish 된 데이터를 Spark Streaming을 이용하여 Message Consume 진행  
- Consume 된 데이터는 각 목적에 따라 Medallion Architecture 구성으로 나누어 저장

---
<p align="center">
  <img src="./00. Local Architecture 구성/03. 시나리오3.png" width="600"/>
</p>

## Fig 3. Kafka Topic을 이용한 Data Hub 구성 (Consumer Group Test)

### 시나리오 3 목적

- Kafka의 Message 기능 테스트 (Pub/Sub)  
- 각 사용 목적에 따른 Kafka Consumer를 구성하여 데이터 적재 및 시각화 (ELK Stack & Grafana)

### 시나리오 3 Workflow

- Local 특정 폴더(`./dummy_data`)에 위치한 모든 JSON 파일을 Kafka의 특정 Topic(`test-topic`)에 Publish 진행  
- Kafka의 특정 토픽(`test-topic`)에 Publish 된 데이터를 여러 Consumer들이 데이터를 Consume하여  
  사용 목적에 따른 데이터 가공 및 적재 (시각화 및 데이터 분석)
