```python
import pyspark.sql.functions as f
from pyspark.sql import SparkSession
import time
import datetime

spark = SparkSession.builder \
    .appName("Hive Test") \
    .config("spark.sql.warehouse.dir", "file:///tmp/warehouse") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hive") \
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.local.type", "hadoop") \
    .config("spark.sql.catalog.local.warehouse", "file:///tmp/warehouse") \
    .config("spark.jars.packages", 
            "org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.2,"
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,"
            "org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.3.0,"
            "mysql:mysql-connector-java:5.1.49,"
            "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,"
            "org.elasticsearch:elasticsearch-spark-30_2.12:8.10.2") \
    .config("spark.sql.hive.metastore.uris", "thrift://hive-metastore:9083") \
    .enableHiveSupport() \
    .getOrCreate()

spark.sql("SHOW DATABASES").show()


from pyspark.sql.types import *

schema = StructType([
    StructField("_id", StringType()),
    StructField("name", StringType()),
    StructField("dob", StringType()),
    StructField("address", MapType(StringType(), StringType())),
    StructField("telephone", StringType()),
    StructField("pets", ArrayType(StringType())),
    StructField("score", DoubleType()),
    StructField("email", StringType()),
    StructField("url", StringType()),
    StructField("description", StringType()),
    StructField("verified", BooleanType()),
    StructField("salary", IntegerType())
])

df = spark.readStream \
    .schema(schema) \
    .option("multiline", "true") \
    .json("./dummy_data")
    
df = df.withColumn("source_file", f.input_file_name())\
        .withColumn("upload_time", f.current_timestamp())
        
query = df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .option("checkpointLocation", "/tmp/checkpoints/iceberg_streaming_checkpoint") \
    .trigger(once=True) \
    .toTable("default.local_stream_test")
    # .toTable("local.dummy_data.example")
    
query.awaitTermination()



spark.sql('''
select * from default.local_stream_test
''').show()

query.stop()
```