# 1. Glue ETL이란?
- Glue ETL Job은 Spark을 커스터마이징을 통한 Glue Spark 엔진을 기반으로 구성
- Pyspark 언어 기반의  데이터 처리가 가능
- Glue Spark은 매직 커맨드 (%)를 이용하여 Configration이 가능
- worker 타입, worker 수, glue version 등

---
# 2. 코드리뷰
- 매직커맨드를 이용한 glue spark configration

<p align="center">
  <img src="./Images/03. glue-spark-config.png" width="600"/>
</p>

```python
%idle_timeout 2880
%glue_version 5.0
%worker_type G.1X
%number_of_workers 2
# %extra_jars s3://datalab-public-bucket/jars/spark-streaming-kinesis-asl_2.13-3.5.5.jar
%extra_jars s3://datalab-public-bucket/jars/spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar
```

- glue spark 자체 내장 패키지 이외 다른 패키지를 이용하여 데이터를 처리해야 할 경우, 외부 jar파일이 필요함
- AWS에서는 s3에 특정 패키지에 대한 jar파일을 업로드하고, 해당 path를 extra_jars에 등록하여 사용하는 방식


---
# 2. Base Package Import
- glue 및 pyspark 관련 패키지를 정상적으로 import 시키게 되면 glue kernel, catalog 옵션이 정상적으로 활성화
- 이전 cell에서 실행한 매직커맨드 중 extra jars에 대한 옵션도 적용된 것 확인

<p align="center">
  <img src="./Images/04. glue-spark-import.png" width="600"/>
</p>

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

from pyspark.sql import SparkSession
sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

spark.conf.set("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog")
spark.conf.set("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
spark.conf.set("spark.sql.catalog.glue_catalog.warehouse", "s3://datalab-public-bucket/warehouse")
spark.conf.set("spark.sql.catalog.glue_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")

job = Job(glueContext)
```
- 추가 Iceberg Format 관련 glue catalog 옵션은 spark.config.set()을 이용하여 추가 세팅 필요

---

# 3. Glue Spark ReadStream / WriteStream
- 데이터를 배치 / 실시간으로 처리할 수 있는 Spark Stream 기능을 통해 데이터를 처리
<p align="center">
  <img src="./Images/05. glue-spark-stream.png" width="600"/>
</p>
```python
kinesis = (
    spark.readStream
    .format("aws-kinesis")
    .option("kinesis.region", "ap-northeast-2")
    .option("kinesis.streamName", "stream_based_on_code")
    .option("kinesis.consumerType", "GetRecords")
    .option("kinesis.endpointUrl", "https://kinesis.ap-northeast-2.amazonaws.com")
    .option("kinesis.startingposition", "LATEST")
    .load()
)

db_name = "dummy_crawl_db"
table_name = "iceberg_v1"
check_point = "s3://datalab-public-bucket/iceberg_stream_test_checkpoint"

write_stream = kinesis.writeStream.format("iceberg")\
            .option("checkpointLocation", f"{check_point}")\
            .option("write-format", "parquet")\
            .outputMode("append")\
            .trigger(once=True)\
            .toTable(f"glue_catalog.{db_name}.{table_name}")
```

- Spark은 데이터 처리에 대한 Checkpoint를 metadata file로 저장하며, 해당 check_point 변수에 위치한 s3 bucket의 폴더 내에 저장됨
- writeStream()함수에서 테이블의 옵션을 지정할 수 있으며, 저장 format은 iceberg table, 파일 형식은 parquet, 적재 방식은 누적으로 적재하도록 옵션을 지정
- toTable()함수를 이용하여 코드기반 Glue Table 생성

---

# 4. 테이블 조회
- spark.sql()함수를 이용하여 테이블에 직접 Query를 보내고, 그 결과를 확인할 수 있음

```python
spark.sql(f'''
SELECT * FROM glue_catalog.{db_name}.{table_name}
''').show()
```

[조회결과]
```python
+--------------------+--------------------+-------------+--------------------+---------------------------+
|                data|          streamName| partitionKey|      sequenceNumber|approximateArrivalTimestamp|
+--------------------+--------------------+-------------+--------------------+---------------------------+
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-10 09:05:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
|[7B 27 6C 61 72 6...|stream_based_on_code|csv-partition|49663921274282015...|       2025-06-11 00:18:...|
+--------------------+--------------------+-------------+--------------------+---------------------------+
only showing top 20 rows
​
```

<p align="center">
  <img src="./Images/06. glue-spark-data-select.png" width="600"/>
</p>

- 이후 data컬럼에 대한 binary 정보를 string으로 변환 및 json 변환 과정을 거쳐 최종적으로 테이블로 구능